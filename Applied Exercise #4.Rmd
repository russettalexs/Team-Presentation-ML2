---
title: 'Applied Exercise #4'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## Applied Exercise #4

### Directions

In this Applied Exercise, we will generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. We will show that  in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. We will then tune each function, and determine which would perform the best on test data using 10-fold cross-validation.

### Generate, Plot Data

We start by setting the seed for reproducibility, and then generating 100 random, positive data points for two columns of the dataframe X. The first
column will have a mean of 2 and a standard deviation of 0.8, while the second will have a mean of 4 and a standard deviation of 2. We will call them X1 and X2, respectively. Finally, we separate the randomly generated data into two classes along a third-degree polynomial, and plot this data. From the plot, we see a clear, non-linear partition between the classes of the randomly distributed data.

```{r}
set.seed(2020)
X <- data.frame(X1 = abs(rnorm(100, 2, 0.8)), X2 = abs(rnorm(100, 4, 2)))

# nonlinear decision boundary

class <-  ifelse(-9/2*(X[,1] - 3/2)*(X[,1] - 2)*(X[,1] - 3.5)+2 - X[2] > 0, 'blue', 'red')  # Yl-Yp > 0 => Point lies above the line

plot(X, xlab='X1', ylab='X2', col = class)

```

### Create the Support Vector Classifier Model

After attaching the necessary package, we use it to generate a supplied vector machine (SVM) model with a linear kernel (also known as a support vector classifier) that uses the data given to find a linear separation between the two classes. We see that more than half of the data points are used as support vectors. 

```{r}
library(e1071)
svm.linear <- svm(class ~., data = data.frame(X, class = as.factor(class)), kernel = 'linear')
summary(svm.linear)

```

### Plot the Linear SVM Model

Next, we plot the support vector classifier model to visually examine its efficacy. The X's on the plot indicate the points that were used as support vectors, while the O's were unused. Comparing this plot to the one above of just the data, it is easy to see that a support vector classifier model does not perform well.

```{r}

plot(svm.linear, data = data.frame(X, class = as.factor(class)), X2~X1, col=c('cyan','pink'))


```

### Generate the Confusion Matrix

Next, we use the model to predict the two classes and generate the confusion matrix relating the predictions to the actual data. We see that the 14 blue points were misclassified as red and 11 red points were misclassified as blue. This puts the error rate at 25%.

```{r}

svm.linear.pred <- predict(svm.linear, X, type='response')
table(class, svm.linear.pred)

```
### Tune the Model

In order to test the model, we can perform 10-fold cross-validation using a range of costs to optimize the model. We see that the tune() function settles on a cost of 0.1, which would not over-fit the model while theoretically improving its performance. However, we see that the best performance is at 0.27% error - higher than what we found without tuning the model. This indicates that the two-class data is not well ft by a support vector classifier model.

```{r}
set.seed(2020)
svm.linear.tune=tune(method = svm, class~., data = data.frame(X, class = as.factor(class)),
                     kernel = "linear", ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
svm.linear.tune

```
### Examine Best Model

Looking at the most effective model found with tuning, we see that the number of support vectors has further increased to two-thirds of the available data points. This means we are likely overfitting our data, yet seeing no return from doing so as we are using an SVM with a linear kernel on data that is split non-linearly.

```{r}

svm.linear.tune$best.model

```

### Plot Best Model

The above is further confirmed by the plot of the best model, which is hardly different from the original support vector classifier model. 

```{r}

plot(svm.linear.tune$best.model, data = data.frame(X, class = as.factor(class)), X2~X1, col=c('cyan','pink'))

```

### Generate Best Model Confusion Matrix

Finally, we use the best model found with tuning to predict the two classes and generate the confusion matrix relating the predictions to the actual data. This is equivalent to the 'test' results in a train/test split. We see that the same number of points were misclassified, putting the error rate at 25%.

```{r}

svm.linear.pred = predict(svm.linear.tune$best.model, X, type=response)
table(class, svm.linear.pred)

```

### Create the Polynomial SVM Model

Next we generate a SVM model with a polynomial kernel that uses the data given to find a polynomial separation between the two classes. Similarly to the support vector classifier model, we see that more than half of the data points are used as support vectors - an early indicator of overfitting. However, we also see that the SVM model correctly found the degree of the polynomial that splits our data, a good sign that the model can improve with tuning.


```{r}

svm.poly <- svm(class ~., data = data.frame(X, class = as.factor(class)), kernel = 'polynomial')
summary(svm.poly) 

```

### Plot the Polynomial SVM Model

Next, we plot the polynomial SVM model to visually examine its efficacy. Comparing this plot to that of the support vector classifier model, we immediately notice a far more similar trend in the model's prediction to the actual data. However, if we compare this plot to that of the original data, we can see that there is still some ways to go before the model accurately predicts the two classes of data.

```{r}

plot(svm.poly, data = data.frame(X, class = as.factor(class)), X2~X1, col=c('cyan','pink'))

```

### Generate the Confusion Matrix

Next, we use the model to predict the two classes and generate the confusion matrix relating the predictions to the actual data. We see that 20 blue points were misclassified as red and 1 red point were misclassified as blue. This puts the error rate at 21% - just under that of the support vector classifier model, which initially had a 25% error rate.

```{r}

svm.poly.pred <- predict(svm.poly, X, type='response')
table(class, svm.poly.pred)

```

### Tune the Polynomial Model

In order to test the model, we can perform 10-fold cross-validation using a range of costs to optimize the model. We see that the tune() function settles on a cost of 10, which should not over-fit the model while improving its performance. Confirming this, we see that the best performance is at 17% error - lower than what we found without tuning the model. This indicates that the two-class data is better fit by the polynomial SVM model.

```{r}

set.seed(2020)
svm.poly.tune=tune(method = svm, class ~., data = data.frame(X, class = as.factor(class)),
                   kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100)))

svm.poly.tune


```

### Examine Polynomial Best Model

Looking at the most effective model found with tuning, we see that the number of support vectors has decreased to just below half of the available data points. This is a good sign that we are not overfitting our data. 

```{r}

svm.poly.tune$best.model

```

### Plot Polynomial Best Model

The plot of the best polynomial SVM model is visibly different than our original polynomial SVM model. However, it seems to stray further from the standard third-degree polynomial function we used to classify our data. This indicates that we may in fact be overfitting our data, as a small concentration of points - all used as support vectors - in one part of the graph seems to be skewing the SVM model.

```{r}

plot(svm.poly.tune$best.model, data = data.frame(X, class = as.factor(class)), X2~X1, col=c('cyan','pink'))

```

### Generate Best Model Confusion Matrix

Finally, we use the best model found with tuning to predict the two classes and generate the confusion matrix relating the predictions to the actual data. We see that fewer total points were misclassified, with only 13 blues misclassified as red while 2 reds were misclassified as blue. This puts the error rate at 15%, a significant improvement from the support vector classifier model, which had a test error rate of 25%.

```{r}

svm.poly.pred.best <- predict(svm.poly.tune$best.model, X, type=response)
table(class, svm.poly.pred.best)

```

### Create the Radial SVM Model

The final model we will generate is a SVM model with a radial kernel. Unlike either of the previous two models, we see that fewer than half of the data points are used as support vectors.

```{r}

svm.radial <- svm(class ~., data = data.frame(X, class = as.factor(class)), kernel = 'radial')
summary(svm.radial) 

```

### Plot the Radial SVM Model

Next, we plot the radial SVM model to visually examine its efficacy. Comparing this plot to that of the support vector classifier model, we immediately notice a far more similar trend in the model's prediction to the actual data - far more pronounced even than the same trend seen in the plot of the polynomial model. If we compare this plot to that of the original data, we see that the radial model is far closer than either of the other two to our true classification.

```{r}

plot(svm.radial, data = data.frame(X, class = as.factor(class)), X2~X1, col=c('cyan','pink'))

```

### Generate the Confusion Matrix

Next, we use the model to predict the two classes and generate the confusion matrix relating the predictions to the actual data. We see that 5 blue points were misclassified as red and 2 red points were misclassified as blue. This puts the error rate at 7% - far below that of either of the first two models, tuned or not.

```{r}

svm.radial.pred <- predict(svm.radial, X, type='response')
table(class, svm.radial.pred)

```

### Tune the Radial Model

Tuning the model, we see that optimal cost is 10 (the same as that of the polynomial model), which should not over-fit the model while improving its performance. Confirming this, we see that the best performance is at 4% error - lower than what we found without tuning the model. This indicates that the radial SVM model performs very well when fitting the two-class, non-linearly-classified data.

```{r}

set.seed(2020)
svm.radial.tune=tune(method = svm, class ~., data = data.frame(X, class = as.factor(class)),
                   kernel = 'radial', ranges=list(cost=c(0.1, 1, 10, 100, 1000)))

svm.radial.tune


```

### Examine Radial Best Model

Looking at the most effective model found with tuning, we see that the number of support vectors has further decreased to just over one quarter of the available data points. This is a good sign that we are not overfitting our data. 

```{r}

svm.radial.tune$best.model

```

### Plot Radial Best Model

The plot of the best radial SVM model is visibly different than our original radial SVM model. It appears to be closer to the trend shown in our original data, though there is a curious, additional blue classification in the top-right corner. Regardless, this model clearly performs better than either the linear or the polynomial SVM model in classifiying our original data.

```{r}

plot(svm.radial.tune$best.model, data = data.frame(X, class = as.factor(class)), X2~X1, col=c('cyan','pink'))

```

### Generate Best Model Confusion Matrix

Finally, we use the best model found with tuning to predict the two classes and generate the confusion matrix relating the predictions to the actual data. We see that even fewer total points were misclassified, with only 1 blue misclassified as red while no reds were misclassified as blue. This puts the error rate at 1%, an incredible improvement from the linear and polynomial SVM models, which had error rates at 25% and 17%, respectively.

```{r}

svm.radial.pred.best = predict(svm.radial.tune$best.model, X, type=response)
table(class, svm.radial.pred.best)

```

### Conclusions

By selecting a random set of data points and classifying these points along a non-linear separation, we have shown that of the three SVM models with different kernels, the support vector classifier model performs the worst when predicting the classifications of our data points. On the other hand, the SVM model with the radial kernel far out-performed either of the other two models, particularly once it had been tuned. Finally, no model required a particularly high cost when tuning, indicating that the model would perform well on further test data.

















